{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18 Pre_Trained Models and Transfer Learning",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1E5M8UMsUTcDuJgtQL_py6bjqSDuVqiLS",
      "authorship_tag": "ABX9TyPOHkANtnwKGXuAd2zpEUBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohailkhan/Acomputer_vision/blob/main/18_Pre_Trained_Models_and_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPV9ErcueOfy"
      },
      "source": [
        "# **Summary**\n",
        "\n",
        "# **1-Using pre-trained model completely**\n",
        "\n",
        "# **2-Using Pre-Trained Model as Feature Extractor processor**\n",
        "\n",
        "1.   Just remove the last layer i.e Final Classification layer ONLY. Leave the Dense and Flatten layers\n",
        "2.   What is in the 2nd last year? It has exctrated features in form of a vector which is attached final Dense layer\n",
        "2.   Hence ,Input=same model.input , but output is the **output** of 2ndlast layer i.e Output= model.layers[-2].output\n",
        "```\n",
        "modelnew=tf.keras.Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
        "```\n",
        "\n",
        "\n",
        "# **3-Using Pre-Trained Model as Feature Extractor processor**\n",
        "\n",
        "1.   Remove the last Dense layers (ofcourse with the Flatten layer before them) and  the final classification layer(with softmax activation. (Strangely,this done by include_top=False)\n",
        "2.   We can use our own input shape in the input\n",
        "3- After removing the flatten,dense and classification layers, we can add our own Flatten , dense and output classification layer (with softmax)\n",
        " We can use our own number of classes in the final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B1u-P1Gdlfv"
      },
      "source": [
        "from numpy import asarray\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras import utils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ74Xt5H_O4L",
        "outputId": "077df116-618d-4841-d517-bde88d7695f0"
      },
      "source": [
        "# load model without output layer\n",
        "from keras.applications.vgg16 import VGG16\n",
        "new_input = Input(shape=(640, 480, 3))\n",
        "model = VGG16(include_top=False, input_tensor=new_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_G4GPurHFsj",
        "outputId": "4726cba3-7986-4929-d9b3-f7ed12274c9b"
      },
      "source": [
        "img='/content/drive/MyDrive/20210309_Datasets/dog.jpg'\n",
        "image=tf.keras.utils.load_img(img,target_size=(224,224))\n",
        "image=np.array(image)\n",
        "image=image.astype('float32')\n",
        "image.shape ,image.mean() , image.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((224, 224, 3), 117.332726, 77.13979)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS9ekvCVQJYl"
      },
      "source": [
        "## Global Standardization (normal with Negative values also)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywk8W5PrQbvO",
        "outputId": "195dc1f6-c056-418f-9f10-0fb9304edfe6"
      },
      "source": [
        "standard=(image - image.mean())/image.std()\n",
        "standard.mean() , standard.std() ,image.mean() , image.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.324885e-08, 1.0, 117.332726, 77.13979)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFrxd701RC5d"
      },
      "source": [
        "# Positive Global Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvdTZEk6RKK0",
        "outputId": "31fd3225-fd2f-444c-9c57-f5e4b9acc579"
      },
      "source": [
        "# just use clip (x , -1.0,1.0) and then shift from -1,1  to 0,1 with 0.5 mean\n",
        "standard_P=( (image - image.mean())/ image.std() )\n",
        "standard_P=np.clip( standard_P, -1.0,1.0)\n",
        "standard_P= (standard_P + 1.0) / 2.0\n",
        "standard_P.mean() , standard_P.std(),standard.mean() , standard.std() ,image.mean() , image.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52296525, 0.3971617, -3.324885e-08, 1.0, 117.332726, 77.13979)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-k4SuHdqXcI"
      },
      "source": [
        "# Local Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPqU9H19qatT",
        "outputId": "3848667f-534c-41d9-8ba1-3ae20dcde180"
      },
      "source": [
        "image.mean(axis=(0,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([118.60417, 117.79375, 115.60025], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdA3VRsNqVGy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGiOyxmNS37L"
      },
      "source": [
        "image=standard_P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8LXOQRbWVoH"
      },
      "source": [
        "# keras.applications.resnet.ResNet50\n",
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOMiSMNiI60o"
      },
      "source": [
        "#reshape image for model...1 sample x W x Ht x Channels\n",
        "#image=image.reshape(1,224,224,3) #or\n",
        "image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFwMSA6_JQyo",
        "outputId": "bf42b7e7-b6db-412d-b67b-664002f89a0e"
      },
      "source": [
        "image.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfW7K--JLpvh",
        "outputId": "fc89e42c-406c-4938-bc95-22d67811fb5f"
      },
      "source": [
        "np.std(image),np.mean(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3971617, 0.52296525)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWtOnxhQ5tEf"
      },
      "source": [
        "image=preprocess_input(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLIcp6SG51V3",
        "outputId": "88c70131-42c6-46a5-fc88-3f31264bda2c"
      },
      "source": [
        "np.std(image),np.mean(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8.181295, -114.276375)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dKOvFSlTYmv"
      },
      "source": [
        "# example of loading the resnet50 model\n",
        "#from keras.applications.resnet import ResNet50\n",
        "from keras.applications.vgg16 import  VGG16 ,decode_predictions,preprocess_input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQG-DuvtLcT2"
      },
      "source": [
        "\n",
        "# summarize the model\n",
        "#model.summary()\n",
        "# plot model architecture\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, to_file='vgg_block.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6fc1d3MWXP"
      },
      "source": [
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-8-H9CLNIhM"
      },
      "source": [
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)\n",
        "# convert the probabilities to class labels\n",
        "label1 = decode_predictions(yhat)\n",
        "# retrieve the most likely result, e.g. highest probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBCYGZAn7Zsf"
      },
      "source": [
        "label1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spITYW50efX4"
      },
      "source": [
        "# **1- Using pre-trained model completely**\n",
        "Note: vgg16 in keras has its own preprocess_input and decode_predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxnHMUXzJT4n"
      },
      "source": [
        "# load model without output layer\n",
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VwrvTe27BG6",
        "outputId": "beb98f9f-7c19-4a05-d14e-1a8e8e2396f9"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "# load an image from file\n",
        "dog='/content/drive/MyDrive/20210309_Datasets/afgdog.jpg'\n",
        "image = load_img(dog, target_size=(224, 224))\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label1 = label[0][0]\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label1[1], label1[2]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 5s 0us/step\n",
            "553476096/553467096 [==============================] - 5s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n",
            "49152/35363 [=========================================] - 0s 0us/step\n",
            "Afghan_hound (99.99%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VIw2OQgQKJ7"
      },
      "source": [
        "layers=[1,2,3,4,5]\n",
        "layers.pop() , layers[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaeaRFGSQ2K7"
      },
      "source": [
        "model=VGG16()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKrOIxUEWTDD"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrIWzHwBTk_K"
      },
      "source": [
        "# **2-Using Pre-Trained Model as Feature Extractor processor**\n",
        "\n",
        "\n",
        "1.   Just remove the last layer i.e Final Classification layer\n",
        "2.   What is in the 2nd last year? It has exctrated features in form of a vector which is attached final Dense layer\n",
        "2.   Hence ,Input=same model.input , but output is the **output** of 2ndlast layer i.e Output= model.layers[-2].output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7BwjbczXooA"
      },
      "source": [
        "h='12345'\n",
        "h[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVk07cPERRws"
      },
      "source": [
        "model.layers[-1] , model.layers[-2],model.layers[-3],model.layers[-4] ,model.layers.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xurPpttVY7VA"
      },
      "source": [
        "import tensorflow as tf\n",
        "modelnew=tf.keras.Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
        "\n",
        "# get the features from the image..first prprocess the image\n",
        "dog='/content/drive/MyDrive/20210309_Datasets/afgdog.jpg'\n",
        "image = load_img(dog, target_size=(224, 224))\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lpp9rnaaz1u"
      },
      "source": [
        "# get the features from the image\n",
        "features1=modelnew.predict(image)\n",
        "print('Prediction shape={} is same as the output layer shape={}'.format(modelnew.layers[-2].output.shape,features1.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6K0AfL0kSpP"
      },
      "source": [
        "print(features1.shape)\n",
        "from pickle import dump\n",
        "dump(features1, open('dog.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csafkhUTmCfa"
      },
      "source": [
        "### Book method for extracting features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlWWTr5mBwc"
      },
      "source": [
        "# the method in the book\n",
        "# remove the output layer\n",
        "model2 = VGG16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElxrCrA4acsm"
      },
      "source": [
        "\n",
        "model2.layers.pop()\n",
        "modelnew2=tf.keras.Model(inputs=model2.inputs,outputs=model2.layers[-1].output)\n",
        "# get extracted features\n",
        "features2 = modelnew2.predict(image)\n",
        "print(features2.shape)\n",
        "# save to file\n",
        "dump(features2, open('dog.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHZM_IYJlsyM"
      },
      "source": [
        "print(features2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExE47v7JlLey"
      },
      "source": [
        "print('Prediction shape={} is same as the output layer shape={}'.format(model.layers[-2].output.shape,features.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCGphIbMR8_m",
        "outputId": "4f6994db-e2d9-4afc-fafe-fc712814d602"
      },
      "source": [
        "model.inputs  , model.layers[-1].output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_3')>],\n",
              " <KerasTensor: shape=(None, 1000) dtype=float32 (created by layer 'predictions')>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbfvOPifUd9"
      },
      "source": [
        "# **3-Using Pre-Trained Model as Feature Extractor processor**\n",
        "\n",
        "\n",
        "1.   Remove the last Dense layers (ofcourse with the Flatten layer before them) and  the final classification layer(with softmax activation. (Strangely,this done by include_top=False)\n",
        "2.   We can use our own input shape in the input\n",
        "3- After removing the flatten,dense and classification layers, we can add our own Flatten , dense and output classification layer (with softmax)\n",
        " We can use our own number of classes in the final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wABSbCvilBRH"
      },
      "source": [
        "model=VGG16(include_top=False,input_shape=(300,300,3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOakZjFslrEi",
        "outputId": "4dec8a07-02ad-4572-fa4d-c07d8783279d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 300, 300, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 150, 150, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 150, 150, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 75, 75, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 75, 75, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 75, 75, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZJ4BA8tJN9",
        "outputId": "1f666638-a701-4206-e9e8-8a62cd37b093"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "# load model without classifier layers\n",
        "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
        "# add new classifier layers\n",
        "flat1 = Flatten()(model.outputs[0])\n",
        "class1 = Dense(1024, activation='relu')(flat1)\n",
        "output = Dense(10, activation='softmax')(class1)\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "# summarize\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 300, 300, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 150, 150, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 150, 150, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 75, 75, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 75, 75, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 75, 75, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 41472)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              42468352  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 57,193,290\n",
            "Trainable params: 57,193,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKF6-SYUucRm",
        "outputId": "26cc9840-2bb5-443b-de3f-3d508642038d"
      },
      "source": [
        "model.outputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 9, 9, 512) dtype=float32 (created by layer 'block5_pool')>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP35v-C7wVOU"
      },
      "source": [
        "## Using some Layers Traianable and some without Training\n",
        "Looking at above summary, support we wish that the shalllow layers are skipped and not trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HQ6EobDwiv8"
      },
      "source": [
        "mode.get_layers('block1_conv1').trainable=False\n",
        "mode.get_layers('block1_conv2').trainable=False\n",
        "mode.get_layers('block1_conv1').trainable=True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}